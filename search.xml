<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flink学习]]></title>
    <url>%2F2019%2F07%2F11%2FFlink%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1. Flink安装和使用Mac os下Flink安装：https://www.jianshu.com/p/17676d34dd35 Flink Python 安装使用：http://www.willmcginnis.com/2015/11/08/getting-started-with-python-and-apache-flink/ 1.1 命令行工具 https://blog.csdn.net/sunnyyoona/article/details/78316406 1.2 数据来源 Flink已经帮我们实现好了一些source functions，已经实现好的大概分为：Collection、TextFile、socket。除此外也可以自定义数据源，addSource - 添加一个新的 source function。 1、基于集合：有界数据集，更偏向于本地测试用 2、基于文件：适合监听文件修改并读取其内容 3、基于 Socket：监听主机的 host port，从 Socket 中获取数据 4、自定义 addSource：大多数的场景数据都是无界的，会源源不断的过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。 从kafka获取数据例子如下： 123456789StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;KafkaEvent&gt; input = env .addSource( new FlinkKafkaConsumer011&lt;&gt;( parameterTool.getRequired("input-topic"), //从参数中获取传进来的 topic new KafkaEventSchema(), parameterTool.getProperties()) .assignTimestampsAndWatermarks(new CustomWatermarkExtractor())); https://zhuanlan.zhihu.com/p/48537579 1.3 算子 参考链接：https://blog.csdn.net/chybin500/article/details/87260869 1.4 数据sink 基于文件的：如 writeAsText()、writeAsCsv()、writeUsingOutputFormat、FileOutputFormat 写到socket： writeToSocket 用于显示的：print、printToErr 自定义Sink： addSink 2. 公司资源Mlink技术分享：http://cf.meitu.com/confluence/pages/viewpage.action?pageId=88978301 3. flink原理原理解析：https://www.cnblogs.com/huiandong/p/10090912.html 时间处理和水印：https://blog.csdn.net/a6822342/article/details/78064815 checkout和savepoint的差异：http://www.whitewood.me/2018/09/06/Flink-Checkpoint-Savepoint-%E5%B7%AE%E5%BC%82/ 反压问题：http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/ 4. 优势 批处理、流处理统一 多种时间类型保证低延迟（解决生产延迟问题） 状态管理（统计计数，已处理数据的窗口，状态机，嵌入式数据库） 5. 参考 阿里flink开发者：http://wuchong.me/archives/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES与Java交互]]></title>
    <url>%2F2019%2F06%2F10%2FES%E4%B8%8EJava%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[1.客户端介绍elasticsearch-rest-high-level-client https://www.elastic.co/guide/en/elasticsearch/client/java-rest/master/java-rest-high-javadoc.html 2.环境搭建2.1 pom.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itqf&lt;/groupId&gt; &lt;artifactId&gt;ElasticSearch&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.3.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.30&lt;/version&gt; &lt;/dependency&gt; &lt;!--Swagger start--&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.2 创建yml配置文件 日志文件12345678910spring.data.elasticsearch.host=192.168.82.193spring.data.elasticsearch.port=9200spring.data.elasticsearch.connectTimeOut=1000spring.data.elasticsearch.socketTimeOut=30000spring.data.elasticsearch.connectionRequestTimeOut=500spring.data.elasticsearch.maxConnectNum=100spring.data.elasticsearch.maxConnectPerRoute=100#logging.path=D:/logs/cachelogging.path=logs/cache springboot的日志文件 spring-logback.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;property name="LOG_PATTERN" value="[ %-5level] [%date&#123;yyyy-MM-dd HH:mm:ss&#125;] %logger&#123;96&#125; [%line] - %msg%n"/&gt; &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt; $&#123;LOG_PATTERN&#125; &lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="FILE-INFO" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;File&gt;$&#123;LOG_PATH&#125;/info.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/info-%d&#123;yyyyMMdd&#125;.log.%i &lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;maxHistory&gt;2&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt; $&#123;LOG_PATTERN&#125; &lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="FILE-ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;File&gt;$&#123;LOG_PATH&#125;/error.log&lt;/File&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;LOG_PATH&#125;/error-%d&#123;yyyyMMdd&#125;.log.%i &lt;/fileNamePattern&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;maxHistory&gt;2&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt; $&#123;LOG_PATTERN&#125; &lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;springProfile name="dev"&gt; &lt;logger name="com.itqf" level="INFO"&gt; &lt;appender-ref ref="FILE-ERROR"/&gt; &lt;appender-ref ref="FILE-INFO"/&gt; &lt;/logger&gt; &lt;/springProfile&gt; &lt;springProfile name="local"&gt; &lt;logger name="com.itqf" level="debug"&gt; &lt;appender-ref ref="FILE-ERROR"/&gt; &lt;appender-ref ref="FILE-INFO"/&gt; &lt;/logger&gt; &lt;/springProfile&gt; &lt;springProfile name="prod"&gt; &lt;logger name="com.itqf" level="INFO"&gt; &lt;appender-ref ref="FILE-ERROR"/&gt; &lt;appender-ref ref="FILE-INFO"/&gt; &lt;/logger&gt; &lt;/springProfile&gt; &lt;root level="info"&gt; &lt;appender-ref ref="CONSOLE"/&gt; &lt;appender-ref ref="FILE-ERROR"/&gt; &lt;appender-ref ref="FILE-INFO"/&gt; &lt;/root&gt; &lt;include resource="org/springframework/boot/logging/logback/base.xml"/&gt; &lt;jmxConfigurator/&gt;&lt;/configuration&gt; 2.3 创建ElasticSearch的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.itqf.config;import org.apache.http.HttpHost;import org.apache.http.client.config.RequestConfig;import org.apache.http.impl.nio.client.HttpAsyncClientBuilder;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestClientBuilder;import org.elasticsearch.client.RestHighLevelClient;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import java.util.ArrayList;/** * @Description: * @Company: 千锋互联 * @Author: 李丽婷 * @Date: 2019/5/25 * @Time: 下午8:59 */@Configurationpublic class ElasticSearchConfig &#123; private static final String SCHEMA="http"; private ArrayList&lt;HttpHost&gt; httpHosts; @Value("$&#123;spring.data.elasticsearch.host&#125;") private String hosts;//集群地址 多个用，隔开 @Value("$&#123;spring.data.elasticsearch.port&#125;") private int port;//集群地址 多个用，隔开 @Value("$&#123;spring.data.elasticsearch.connectTimeOut&#125;") private int connectionTimeOut=1000; @Value("$&#123;spring.data.elasticsearch.socketTimeOut&#125;") private int socketTimeOut; @Value("$&#123;spring.data.elasticsearch.connectionRequestTimeOut&#125;") private int connectionRequestTimeOut; @Value("$&#123;spring.data.elasticsearch.maxConnectNum&#125;") private int maxConnectNum; @Value("$&#123;spring.data.elasticsearch.maxConnectPerRoute&#125;") private int maxConnectPerRoute; private RestClientBuilder builder; @Bean @ConditionalOnMissingBean(RestHighLevelClient.class) public RestHighLevelClient client()&#123; httpHosts = new ArrayList&lt;&gt;(); String[] hostsStr = hosts.split(","); for (String s : hostsStr) &#123; httpHosts.add(new HttpHost(s,port,SCHEMA)); &#125; builder = RestClient.builder(httpHosts.toArray(new HttpHost[0])); RestHighLevelClient client = new RestHighLevelClient(builder); return client; &#125; //异步httpclient的连接延时配置 public void setConnectTimeOutConfig()&#123; builder.setRequestConfigCallback(new RestClientBuilder.RequestConfigCallback() &#123; @Override public RequestConfig.Builder customizeRequestConfig(RequestConfig.Builder requestConfigBuilder) &#123; requestConfigBuilder.setConnectionRequestTimeout(connectionTimeOut); requestConfigBuilder.setSocketTimeout(socketTimeOut); requestConfigBuilder.setConnectionRequestTimeout(connectionRequestTimeOut); return requestConfigBuilder; &#125; &#125;); &#125; //异步httpclient连接数量配置 public void setMutiConnectionConfig()&#123; builder.setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() &#123; @Override public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpAsyncClientBuilder) &#123; httpAsyncClientBuilder.setMaxConnTotal(maxConnectNum); httpAsyncClientBuilder.setMaxConnPerRoute(maxConnectPerRoute); return httpAsyncClientBuilder; &#125; &#125;); &#125;&#125; 2.4 编写工具类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.itqf.utils;import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;import org.elasticsearch.common.xcontent.XContentBuilder;import org.elasticsearch.common.xcontent.json.JsonXContent;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.index.query.TermQueryBuilder;import org.elasticsearch.search.builder.SearchSourceBuilder;import java.io.IOException;/** * @Description: * @Company: 千锋互联 * @Author: 李丽婷 * @Date: 2019/5/25 * @Time: 下午10:15 */public class ESUtils &#123; public static void buildIndexMapping(CreateIndexRequest request, String type) throws IOException &#123; XContentBuilder mappingBuilder = JsonXContent.contentBuilder() .startObject() .startObject("properties") .startObject("title") .field("type","keyword") .field("index","true") .endObject() .startObject("author") .field("type","keyword") .field("index","true") .endObject() .startObject("price") .field("type","float") .field("index",true) .endObject(); //text（分词），keyword（不分词）一个用于全文检索，一个用于聚合和排序。 request.mapping(type,mappingBuilder); &#125; public static SearchSourceBuilder getSearchBulder(String searchParams)&#123; //JSONObject jsonObject = JSONObject.parseObject(searchParams); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); TermQueryBuilder termQueryBuilder = null; if (searchParams!=null)&#123; termQueryBuilder = QueryBuilders.termQuery("title",searchParams); &#125; BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); if (termQueryBuilder!=null)&#123; queryBuilder.must(termQueryBuilder); &#125; searchSourceBuilder.query(queryBuilder); return searchSourceBuilder; &#125; //设置分片 public static void buildSetting(CreateIndexRequest request)&#123; request.settings(Settings.builder().put("index.number_of_shards",3)); request.settings(Settings.builder().put("index.number_or_replicas",2)); &#125;&#125; 2.5. 编写service123456789101112131415161718192021222324252627282930313233343536package com.itqf.service;import java.io.IOException;import java.util.List;import java.util.Map;/** * @Description: * @Company: 千锋互联 * @Author: 李丽婷 * @Date: 2019/5/25 * @Time: 下午9:37 */public interface SearchService &#123; //创建索引 public void createIndex() throws IOException; //删除索引 public void delIndex(String indexName) throws IOException; //索引是否存在 public boolean existsIndex(String index ) throws IOException; //新增document public void add(String json ) throws IOException; //新增document 并把自己的业务id作为主键 public void add(String json,String id ) throws IOException; //满足条件的记录 public long count(String params ) throws IOException; //高级查询 public List&lt;Map&gt; search(String searchParams) throws IOException; //全查 public void searchAll() throws IOException; //删除文档 public void deleteDocument() throws IOException;&#125; 2.6 编写实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193package com.itqf.service.impl;import com.itqf.service.SearchService;import com.itqf.utils.SearchUtils;import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;import org.elasticsearch.action.admin.indices.get.GetIndexRequest;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.common.settings.Settings;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.io.IOException;import java.util.List;import java.util.Map;/** * @Description: * @Company: 千锋互联 * @Author: 李丽婷 * @Date: 2019/5/25 * @Time: 下午9:41 */@Service("searchService")public class SearchServiceImpl implements SearchService &#123; private final static Logger log = LoggerFactory.getLogger(SearchServiceImpl.class); private String index = "bookstore"; private String indexType="books"; @Autowired private RestHighLevelClient highLevelClient; @Override public void createIndex() throws IOException &#123; if (!existsIndex(index))&#123; CreateIndexRequest request = new CreateIndexRequest(index); //创建分片和副本 ESUtils.buildSetting(request); ESUtils.buildIndexMapping(request,indexType); String s = request.index(); CreateIndexResponse response = highLevelClient. indices().create(request,RequestOptions.DEFAULT); System.out.println(response.toString()); System.out.println(response.isAcknowledged()); &#125;else&#123; log.warn("索引：&#123;&#125; 已经存在无能再次创建",index); &#125; &#125; @Override public void deleteDocument() throws IOException &#123; DeleteRequest request = new DeleteRequest( index, indexType, "3BRIkmsB2pqzuaP0KicC"); DeleteResponse response = highLevelClient.delete(request); System.out.println(response+""+response.getResult()); &#125; @Override public void searchAll() throws IOException&#123; SearchRequest searchRequest = new SearchRequest(); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchAllQuery()); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println(hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; @Override public void delIndex(String indexName) throws IOException &#123; DeleteIndexRequest request = new DeleteIndexRequest(indexName); AcknowledgedResponse response = highLevelClient.indices().delete(request,RequestOptions.DEFAULT); log.info(response.toString()); &#125; @Override public boolean existsIndex(String index) throws IOException &#123; GetIndexRequest request = new GetIndexRequest(); request.indices(index); boolean exists = highLevelClient.indices().exists(request,RequestOptions.DEFAULT); log.debug("existsIndex:"+exists); return exists; &#125; @Override public void add(String json) throws IOException &#123; IndexRequest indexRequest = new IndexRequest(index,indexType); indexRequest.source(json,XContentType.JSON); IndexResponse response = highLevelClient.index(indexRequest); log.debug("add:"+ JSON.toJSON(indexRequest)); &#125; @Override public long count(String params) throws IOException &#123; SearchSourceBuilder searchSourceBuilder = ESUtils.getSearchBuilder(params); SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSourceBuilder); //3、发送请求 SearchResponse searchResponse = highLevelClient.search(searchRequest); long total = searchResponse.getHits().getTotalHits(); return total; &#125; @Override public List&lt;Map&gt; search(String searchParams) throws IOException &#123; boolean isShowHighLight = false; ArrayList&lt;Map&gt; list = new ArrayList&lt;&gt;(); SearchSourceBuilder searchSourceBuilder = ESUtils.getSearchBuilder(searchParams); JSONObject jsonObject = JSONObject.parseObject(searchParams); Integer start = jsonObject.getInteger("start"); Integer rows = jsonObject.getInteger("rows"); searchSourceBuilder.from(start); searchSourceBuilder.size(rows); String highLightPreTag = jsonObject.getString("highLightPreTag"); String highLightPostTag = jsonObject.getString("highLightPostTag"); // 高亮设置 HighlightBuilder highlightBuilder = new HighlightBuilder(); highlightBuilder.requireFieldMatch(false).field("title").preTags(highLightPreTag).postTags(highLightPostTag); searchSourceBuilder.highlighter(highlightBuilder); SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSourceBuilder); //3、发送请求 SearchResponse searchResponse = highLevelClient.search(searchRequest); //处理搜索命中文档结果 SearchHits hits = searchResponse.getHits(); long totalHits = hits.getTotalHits(); float maxScore = hits.getMaxScore(); log.info("totalHits = " + totalHits); SearchHit[] searchHits = hits.getHits(); for (SearchHit hit : searchHits) &#123; String index = hit.getIndex(); String type = hit.getType(); String id = hit.getId(); float score = hit.getScore(); //取_source字段值 String sourceAsString = hit.getSourceAsString(); //取成json串 Map&lt;String, Object&gt; sourceAsMap = hit.getSourceAsMap(); // 取成map对象 log.info("index:" + index + " type:" + type + " id:" + id); log.info(sourceAsString); //取高亮结果 Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields(); HighlightField highlight = highlightFields.get("title"); if (highlight != null) &#123; Text[] fragments = highlight.fragments(); //多值的字段会有多个值 if (fragments != null) &#123; String fragmentString = fragments[0].string(); log.info("title highlight : " + fragmentString); //可用高亮字符串替换上面sourceAsMap中的对应字段返回到上一级调用 sourceAsMap.put("requestContent", fragmentString); log.info(JSON.toJSONString(sourceAsMap)); list.add(sourceAsMap); &#125; &#125; &#125; StringBuilder info = new StringBuilder(); info.append(",index：").append(index); info.append(",type：").append(indexType); info.append(",查询语句：").append(searchParams); info.append(",开始位置：").append(start); info.append(",显示行数：").append(rows); info.append("显示的指定字段为：显示全部字段"); info.append(",高亮显示的字段为：title,高亮显示前缀：" + highLightPreTag + ",高亮显示后缀：" + highLightPostTag); log.info(list.size() == 0 ? info + "，本次无查询结果！" : info + "，此次查询共有" + totalHits + "条记录,分页显示:" + list.size() + "条。"); log.info("================================================================================"); log.info("query result:" + JSON.toJSONString(list)); return list; &#125;&#125; 2.7 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.itqf.test;import com.alibaba.fastjson.JSON;import com.itqf.AppStart;import com.itqf.pojo.Books;import com.itqf.service.SearchService;import com.itqf.service.impl.SearchServiceImpl;import org.junit.Test;import org.junit.runner.RunWith;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.io.IOException;import java.util.HashMap;import java.util.List;import java.util.Map;@RunWith(SpringRunner.class)@SpringBootTest(classes = AppStart.class)public class SearchServiceTest &#123; private final static Logger log = LoggerFactory.getLogger(SearchServiceImpl.class); @Autowired private SearchService searchService; @Test public void testCreateIndex() throws IOException &#123; searchService.createIndex(); &#125; @Test public void testDelIndex() throws IOException &#123; searchService.deleteIndex("bookstore"); &#125;// @Test public void testAddData()&#123; for(int i=0; i &lt; 500; i++)&#123; Books books = new Books(i+1,"十万个为什么","柠檬",11); try &#123;// Thread.sleep(2000); log.debug(books.toString()); searchService.add(JSON.toJSONString(books)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Test public void testSearch() throws IOException, InterruptedException &#123; HashMap&lt;String ,Object&gt; searchPara = new HashMap&lt;String ,Object&gt;(); searchPara.put("title","十万个为什么"); searchPara.put("start",0); searchPara.put("rows",20); searchPara.put("highLightPreTag", "&lt;b&gt;"); searchPara.put("highLightPostTag", "&lt;/b&gt;"); String str =JSON.toJSONString(searchPara); List&lt;Map&gt; list= searchService.search(JSON.toJSONString(searchPara)); System.out.println(list); &#125;&#125; 3 查询3.1 全查 match_all12345678910111213141516@Override public void queryAll() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchAllQuery()); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println(hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; 3.2 匹配查询 match1234567891011121314151617@Override public void queryMatch() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchQuery("title","十万个 为什么").operator(Operator.AND)); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 3.3 多字段查询1234567891011121314151617@Override public void queryMulitMatch() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.multiMatchQuery("十万个","title","author")); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 3.4 多词条匹配 terms 和 字段过滤1234567891011121314151617181920@Override public void queryTerms() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.termQuery("author.keyword","柠檬")); String s1 [] = &#123;"title","price"&#125;; FetchSourceContext fetchSourceContext = new FetchSourceContext(true,s1,null); searchSourceBuilder.fetchSource(fetchSourceContext); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 4 高级查询4.1 bool组合123456789101112131415161718192021@Override public void queryByBool() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); queryBuilder.must(QueryBuilders.matchQuery("title","十万个")); queryBuilder.mustNot(QueryBuilders.matchQuery("author","十万个")); searchSourceBuilder.query(queryBuilder ); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 4.2 范围12345678910111213141516171819@Override public void queryByRange() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder("price"); rangeQueryBuilder.gte(10).lte(1000); searchSourceBuilder.query(rangeQueryBuilder ); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 4.3 模糊查询12345678910111213141516171819202122@Override public void queryByFuzzy() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); FuzzyQueryBuilder fuzzyQueryBuilder = QueryBuilders.fuzzyQuery("title","javA"); fuzzyQueryBuilder.fuzziness(Fuzziness.AUTO); fuzzyQueryBuilder.prefixLength(0); searchSourceBuilder.query(fuzzyQueryBuilder ); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 4.4 排序12345678910111213141516171819@Override public void queryBySort() throws IOException&#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.sort("price",SortOrder.ASC); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; System.out.println("结果"+hit.getSourceAsMap()); &#125; &#125; 4.5 聚合查询1234567891011121314151617181920212223242526272829public void aggration() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); //按照作者分组 TermsAggregationBuilder aggregation = AggregationBuilders. terms("author_name").field("author"); //求出每个作者出版的书籍的平均价格 aggregation.subAggregation(AggregationBuilders.avg("price_avg") .field("price")); searchSourceBuilder.aggregation(aggregation); //searchSourceBuilder.sort("price",SortOrder.ASC); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); Aggregations aggregations = searchResponse.getAggregations(); Terms terms = aggregations.get("author_name"); List&lt;? extends Terms.Bucket&gt; buckets = terms.getBuckets(); for (int i = 0; i &lt; buckets.size(); i++) &#123; Terms.Bucket elasticBucket = buckets.get(i); Object key = elasticBucket.getKey(); Avg averageAge = elasticBucket.getAggregations().get("price_avg"); Double avg = averageAge.getValue(); int intAvg = avg.intValue(); System.out.println(intAvg+"-------"+key); &#125; &#125; 4.6 分页查询+高亮1234567891011121314151617181920212223242526272829303132333435363738394041@Override public void queryByPage() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(); searchRequest.types("bookstore"); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.from(10); searchSourceBuilder.size(20); //条件 MatchQueryBuilder queryBuilder = new MatchQueryBuilder("title","十万个"); // 高亮设置 HighlightBuilder highlightBuilder = new HighlightBuilder(); highlightBuilder.requireFieldMatch(false).field("title"). preTags("&lt;em style='color:red;'&gt;").postTags("&lt;/em&gt;"); searchSourceBuilder.highlighter(highlightBuilder); searchSourceBuilder.query( queryBuilder); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = highLevelClient.search(searchRequest,RequestOptions.DEFAULT); SearchHits hits = searchResponse.getHits(); System.out.println("count:"+hits.totalHits); SearchHit[] h = hits.getHits(); for (SearchHit hit : h) &#123; //得到高亮显示的集合 Map&lt;String, HighlightField&gt; map = hit.getHighlightFields(); HighlightField highlightField = map.get("title"); // System.out.println("高"+map); if (highlightField!=null)&#123; System.out.println(highlightField.getName()); Text[] texts = highlightField.getFragments(); System.out.println("高亮显示结果"+texts[0]); &#125; System.out.println("普通字段结果"+hit.getSourceAsMap()); &#125; &#125;]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES搭建及简单使用]]></title>
    <url>%2F2019%2F06%2F10%2FES%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.Elasticsearch介绍和安装在企业项目中，面对复杂的搜索业务和数据量，使用传统数据库搜索就显得力不从心（索引失效 ，全表扫描 explan），一般我们都会使用全文检索技术，比如Solr，Elasticsearch。(底层lucene：倒排索引) 要讲的是全文检索技术：Elasticsearch。 1.1.简介1.1.1.ElasticElastic官网：https://www.elastic.co/cn/ Elastic有一条完整的产品线及解决方案：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。 1.1.2.ElasticsearchElasticsearch官网：https://www.elastic.co/cn/products/elasticsearch 如上所述，Elasticsearch具备以下特点： 分布式，无需人工搭建集群（solr就需要人为配置，使用Zookeeper作为注册中心） Restful风格，一切API都遵循Rest原则，容易上手 /name/华为 近实时搜索，数据更新在Elasticsearch中几乎是完全同步的。 1.1.3.版本我们使用6.3.0 需要虚拟机JDK1.8及以上 1.2.安装和配置为了模拟真实场景，我们将在linux下安装Elasticsearch。 1.2.1.新建一个用户es出于安全考虑，elasticsearch默认不允许以root账号运行。 创建用户： 1useradd es 设置密码： 1passwd es 切换用户： 1su - es 1.2.2.上传安装包,并解压我们将安装包上传到：/home/es目录 解压缩： 1tar -zxvf elasticsearch-6.2.4.tar.gz 我们把目录重命名： 1mv elasticsearch-6.3.0/ elasticsearch 1.2.3.修改配置cd /home/es/elasticsearch/config 我们进入config目录：cd config 需要修改的配置文件有两个：elasticsearch.yml和jvm.options jvm.options Elasticsearch基于Lucene的，而Lucene底层是java实现，因此我们需要配置jvm参数。 编辑jvm.options： 1vim jvm.options 默认配置如下： 12-Xms1g-Xmx1g 内存占用太多了，我们调小一些： 12-Xms512m-Xmx512m elasticsearch.yml 1vim elasticsearch.yml 修改数据和日志目录： 12path.data: /home/es/elasticsearch/data # 数据目录位置path.logs: /home/es/elasticsearch/logs # 日志目录位置 我们把data和logs目录修改指向了elasticsearch的安装目录。但是这两个目录并不存在，因此我们需要创建出来。 进入elasticsearch的根目录，然后创建： 123[es@localhost elasticsearch]$ pwd/home/es/elasticsearch[es@localhost elasticsearch]$ mkdir data logs 修改绑定的ip： 1network.host: 0.0.0.0 # 绑定到0.0.0.0，允许任何ip来访问 默认只允许本机访问，修改为0.0.0.0后则可以远程访问 目前我们是做的单机安装，如果要做集群，只需要在这个配置文件中添加其它节点信息即可。 elasticsearch.yml的其它可配置信息： 属性名 说明 cluster.name 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name 节点名，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 path.conf 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开 path.logs 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock 设置为true可以锁住ES使用的内存，避免内存进行swap network.host 设置bind_host和publish_host，设置为0.0.0.0允许外网访问 http.port 设置对外服务的http端口，默认为9200。 transport.tcp.port 集群结点之间通信端口 discovery.zen.ping.timeout 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些 discovery.zen.minimum_master_nodes 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2 1.3.运行进入elasticsearch/bin目录，可以看到下面的执行文件： 然后输入命令： 1./elasticsearch 发现报错了，启动失败。 1.3.1.错误1：内核过低 我们使用的是centos6，其linux内核版本为2.6。而Elasticsearch的插件要求至少3.5以上版本。不过没关系，我们禁用这个插件即可。 修改elasticsearch.yml文件，在最下面添加如下配置： 1bootstrap.system_call_filter: false 然后重启 1.3.2.错误2：文件权限不足再次启动，又出错了： 我们用的是es用户，而不是root，所以文件权限不足。 首先用root用户登录。 然后修改配置文件: 1vim /etc/security/limits.conf 添加下面的内容： 1234567* soft nofile 65536* hard nofile 131072* soft nproc 4096* hard nproc 4096 1.3.3.错误3：线程数不够刚才报错中，还有一行： 1[1]: max number of threads [1024] for user [es] is too low, increase to at least [4096] 这是线程数不够。 继续修改配置： 1vim /etc/security/limits.d/20-nproc.conf 修改下面的内容： 1* soft nproc 1024 改为： 1* soft nproc 4096 1.3.4.错误4：进程虚拟内存1[3]: max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] vm.max_map_count：限制一个进程可以拥有的VMA(虚拟内存区域)的数量，继续修改配置文件， ： 1vim /etc/sysctl.conf 添加下面内容： 1vm.max_map_count=655360 然后执行命令： 1sysctl -p 1.3.5.重启终端窗口所有错误修改完毕，一定要重启你的虚拟机终端，否则配置无效。 1.3.6.启动再次启动，终于成功了！ 可以看到绑定了两个端口: 9300：集群节点间通讯接口 9200：客户端访问接口 我们在浏览器中访问：http://192.168.82.193:9200 1.4.安装kibana1.4.1.什么是Kibana？ Kibana是一个基于Node.js的Elasticsearch索引库数据统计工具，可以利用Elasticsearch的聚合功能，生成各种图表，如柱形图，线状图，饼图等。 而且还提供了操作Elasticsearch索引数据的控制台，并且提供了一定的API提示，非常有利于我们学习Elasticsearch的语法。 1.4.2.安装kibana版本与elasticsearch保持一致，也是6.3.0解压到特定目录即可 1.4.3.配置运行 配置 进入安装目录下的config目录，修改kibana.yml文件： 修改elasticsearch服务器的地址： 12elasticsearch.url: &quot;http://192.168.82.193:9200&quot;server.host: &quot;0.0.0.0&quot; 运行 进入安装目录下的bin目录启动： 1./kibana 发现kibana的监听端口是5601 我们访问：http://193.168.82.193:5601 1.4.4.控制台选择左侧的DevTools菜单，即可进入控制台页面： 在页面右侧，我们就可以输入请求，访问Elasticsearch了。 1.5.安装ik分词器Lucene的IK分词器早在2012年已经没有维护了，现在我们要使用的是在其基础上维护升级的版本，并且开发为ElasticSearch的集成插件了，与Elasticsearch一起维护升级，版本也保持一致，最新版本：6.3.0 1.5.1.安装上传课前资料中的zip包，解压到Elasticsearch目录的plugins目录中： 使用unzip命令解压： 1unzip elasticsearch-analysis-ik-6.3.0.zip -d ik-analyzer 然后重启elasticsearch： 1.5.2.测试大家先不管语法，我们先测试一波。 在kibana控制台输入下面的请求： http://192.168.82.188:9200/_analyze 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, // 中文分词 &quot;text&quot;: &quot;李老师很帅&quot;&#125; 运行得到结果： 还有一种聪明的分词方式： 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;李老师很帅&quot;&#125; 结果： 1.7.APIElasticsearch提供了Rest风格的API，即http请求接口，而且也提供了各种语言的客户端API 1.7.1.Rest风格API文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 作业:了解elasticsearch支持的同义词 土豆 洋芋 马铃薯 西红柿 番茄 1.7.2.客户端APIElasticsearch支持的客户端非常多：https://www.elastic.co/guide/en/elasticsearch/client/index.html 点击Java Rest Client后，你会发现又有两个： Low Level Rest Client是低级别封装，提供一些基础功能，但更灵活 High Level Rest Client，是在Low Level Rest Client基础上进行的高级别封装，功能更丰富和完善，而且API会变的简单 1.7.3.如何学习建议先学习Rest风格API，了解发起请求的底层实现，请求体格式等。 2.操作索引2.1.基本概念Elasticsearch也是基于Lucene的全文检索库，本质也是存储数据，很多概念与MySQL类似的。 对比关系： 索引库（indices）——————————–Databases 数据库 类型（type）-----------------------------Table 数据表 文档（Document）----------------Row 行 字段（Field）-------------------Columns 列详细说明： 概念 说明 索引库（indices) indices是index的复数，代表许多的索引， 类型（type） 类型是模拟mysql中的table概念，一个索引库下可以有不同类型的索引，比如商品索引，订单索引，其数据格式不同。不过这会导致索引库混乱，因此未来版本中会移除这个概念 文档（document） 存入索引库原始的数据。比如每一条商品信息，就是一个文档 字段（field） 文档中的属性 映射配置（mappings） 字段的数据类型、属性、是否索引、是否存储等特性 是不是与Lucene和solr中的概念类似。 另外，在SolrCloud中，有一些集群相关的概念，在Elasticsearch也有类似的： 索引集（Indices，index的复数）：逻辑上的完整索引 分片（shard）：数据拆分后的各个部分 副本（replica）：每个分片的复制 要注意的是：Elasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。 2.2.创建索引2.2.1.语法Elasticsearch采用Rest风格API，因此其API就是一次http请求，你可以用任何工具发起http请求 创建索引的请求格式： 请求方式：PUT 请求路径：/索引库名 请求参数：json格式： 123456&#123; "settings": &#123; "number_of_shards": 3, "number_of_replicas": 2 &#125;&#125; settings：索引库的设置 number_of_shards：分片数量 number_of_replicas：副本数量 http://192.168.82.188:9200/qf 2.2.2.测试我们先用postman测试来试试 可以看到索引创建成功了。 2.2.3.使用kibana创建kibana的控制台，可以对http请求进行简化，示例： 相当于是省去了elasticsearch的服务器地址 而且还有语法提示，非常舒服。 2.3.查看索引设置 语法 Get请求可以帮我们查看索引信息，格式： 1GET /索引库名 或者，我们可以使用*来查询所有索引库配置： 2.4.删除索引删除索引使用DELETE请求 语法 1DELETE /索引库名 当然，我们也可以用HEAD请求，查看索引是否存在： HEAD qianfeng 2.5.映射配置索引有了，接下来肯定是添加数据。但是，在添加数据之前必须定义映射。 什么是映射？ 映射是定义文档的过程，文档包含哪些字段，这些字段是否保存，是否索引，是否分词等 只有配置清楚，Elasticsearch才会帮我们进行索引库的创建（不一定） 2.5.1.创建映射字段 语法 请求方式依然是PUT 12345678910111213141516171819PUT /索引库名/_mapping/类型名称&#123; &quot;properties&quot;: &#123; &quot;字段名&quot;: &#123; &quot;type&quot;: &quot;类型&quot;, &quot;index&quot;: true， &quot;store&quot;: true， &quot;analyzer&quot;: &quot;分词器&quot; &#125; &#125;&#125;put /qf/_mapping/tb_item&#123; &quot;properties&quot;:&#123; &quot;type&quot;: &quot;index&quot;： &quot;store&quot;: &#125;&#125; 类型名称：就是前面讲的type的概念，类似于数据库中的不同表字段名：任意填写 ，可以指定许多属性，例如： type：类型，可以是text、long、short、date、integer、object等 index：是否索引，默认为true store：是否存储，默认为false analyzer：分词器，这里的ik_max_word即使用ik分词器 示例 发起请求： 1234567891011121314151617181920212223POST qf/_mapping/items&#123; "properties": &#123; "title":&#123; "type":"text", "analyzer":"ik_max_word" &#125;, "sellpoint":&#123; "type": "text", "index":true, "store": true &#125; , "images":&#123; "type":"keyword", "index":false &#125;, "price":&#123; "type":"float" &#125; &#125;&#125; 响应结果： 123&#123; &quot;acknowledged&quot;: true&#125; 2.5.2.查看映射关系 语法： 1GET /索引库名/_mapping 示例： 1GET /qf/_mapping 响应： 12345678910111213141516171819202122232425&#123; "qf": &#123; "mappings": &#123; "items": &#123; "properties": &#123; "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "float" &#125;, "sellpoint": &#123; "type": "text", "store": true &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; 2.5.3.字段属性详解2.5.3.1.typeElasticsearch中支持的数据类型非常丰富： 我们说几个关键的： String类型，又分两种： text：可分词，不可参与聚合 keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合 Numerical：数值类型，分两类 基本数据类型：long、interger、short、byte、double、float、half_float 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。 2.5.3.2.indexindex影响字段的索引情况。 true：字段会被索引，则可以用来进行搜索。默认值就是true false：字段不会被索引，不能用来搜索 index的默认值就是true，也就是说你不进行任何配置，所有字段都会被索引。 但是有些字段是我们不希望被索引的，比如商品的图片信息，就需要手动设置index为false。 2.5.3.3.store是否将数据进行额外存储。 在学习lucene和solr时，我们知道如果一个字段的store设置为false，那么在文档列表中就不会有这个字段的值，用户的搜索结果中不会显示出来。 但是在Elasticsearch中，即便store设置为false，也可以搜索到结果。 原因是Elasticsearch在创建文档索引时，会将文档中的原始数据备份，保存到一个叫做_source的属性中。而且我们可以通过过滤_source来选择哪些要显示，哪些不显示。 而如果设置store为true，就会在_source以外额外存储一份数据，多余，因此一般我们都会将store设置为false，事实上，store的默认值就是false。 2.5.3.4.boost激励因子，这个与lucene中一样 其它的不再一一讲解，用的不多，大家参考官方文档： 2.6.新增数据2.6.1.随机生成id通过POST请求，可以向一个已经存在的索引库中添加数据。 语法： 1234POST /索引库名/类型名&#123; &quot;key&quot;:&quot;value&quot;&#125; 示例： 1234567POST /qf/items/&#123; "title":"oppo", "sellpoint":"拍照更更清晰", "images":"http://www.images.com/0001.jpg", "price":1299.00&#125; 响应： 1234567891011121314&#123; "_index": "qf", "_type": "items", "_id": "c9uy1GoBht3nCF-2YSOO", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "_seq_no": 0, "_primary_term": 2&#125; 通过kibana查看数据： 123456get _search&#123; "query":&#123; "match_all":&#123;&#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142&#123; "took": 19, "timed_out": false, "_shards": &#123; "total": 7, "successful": 7, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 2, "max_score": 1, "hits": [ &#123; "_index": ".kibana", "_type": "doc", "_id": "config:6.3.0", "_score": 1, "_source": &#123; "type": "config", "updated_at": "2019-05-16T09:45:28.599Z", "config": &#123; "buildNum": 17230, "telemetry:optIn": true &#125; &#125; &#125;, &#123; "_index": "qf", "_type": "items", "_id": "c9uy1GoBht3nCF-2YSOO", "_score": 1, "_source": &#123; "title": "oppo", "sellpoint": "拍照更更清晰", "images": "http://www.images.com/0001.jpg", "price": 1299 &#125; &#125; ] &#125;&#125; _source：源文档信息，所有的数据都在里面。 _id：这条文档的唯一标示，与文档自己的id字段没有关联 2.6.2.自定义id如果我们想要自己新增的时候指定id，可以这么做： 1234POST /索引库名/类型/id值&#123; ...&#125; 示例： 1234567POST /qf/items/2&#123; "title": "oppo002", "sellpoint": "拍照更更清晰", "images": "http://www.images.com/0002.jpg", "price": 1299&#125; 得到的数据： 1234567891011121314&#123; "_index": "qf", "_type": "items", "_id": "2", "_version": 1, "result": "created", "_shards": &#123; "total": 3, "successful": 1, "failed": 0 &#125;, "_seq_no": 0, "_primary_term": 2&#125; 2.6.3.智能判断在学习Solr时我们发现，我们在新增数据时，只能使用提前配置好映射属性的字段，否则就会报错。 不过在Elasticsearch中并没有这样的规定。 事实上Elasticsearch非常智能，你不需要给索引库设置任何mapping映射，它也可以根据你输入的数据来判断类型，动态添加数据映射。 测试一下： 123456789POST /qf/items/3&#123; "title":"apple7plus", "sellpoint":"", "images":"http://www.images.com/003.jpg", "price":1899.00, "count": 20, "status":1&#125; 结果： 1234567891011121314&#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 3&#125; 新增成功，额外添加了count库存，和status状态两个字段。 执行查询： 123456GET _search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 7, &quot;successful&quot;: 7, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;.kibana&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;config:6.3.0&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;type&quot;: &quot;config&quot;, &quot;updated_at&quot;: &quot;2019-05-16T09:45:28.599Z&quot;, &quot;config&quot;: &#123; &quot;buildNum&quot;: 17230, &quot;telemetry:optIn&quot;: true &#125; &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;apple7plus&quot;, &quot;sellpoint&quot;: &quot;&quot;, &quot;images&quot;: &quot;http://www.images.com/003.jpg&quot;, &quot;price&quot;: 1899, &quot;count&quot;: 20, &quot;status&quot;: 1 &#125; &#125; ] &#125;&#125; 查看映射关系 1GET qf/_mapping 结果： 12345678910111213141516171819202122232425262728293031&#123; &quot;qf&quot;: &#123; &quot;mappings&quot;: &#123; &quot;items&quot;: &#123; &quot;properties&quot;: &#123; &quot;count&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;images&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;sellpoint&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true &#125;, &quot;status&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125; &#125;&#125; 2.7.修改数据把刚才新增的请求方式改为PUT，就是修改了。不过修改必须指定id id对应的文档存在，则修改 id对应的文档不存在，则新增 比如，我们把id为3的数据进行修改： 12345678PUT /qf/items/3&#123; &quot;title&quot;:&quot;apple7&quot;, &quot;images&quot;:&quot;http://www.images.com/003.jpg&quot;, &quot;price&quot;:4500.00, &quot;count&quot;: 50, &quot;status&quot;:1&#125; 结果： 1234567891011121314&#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 1, &quot;_primary_term&quot;: 3&#125; 我们发现result后面接的是：”updated”，说明跟新成功。 再次查询 1GET qf/items/3 结果： 1234567891011121314&#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;apple7&quot;, &quot;images&quot;: &quot;http://www.images.com/003.jpg&quot;, &quot;price&quot;: 4500, &quot;count&quot;: 50, &quot;status&quot;: 1 &#125;&#125; 2.8.删除数据删除使用DELETE请求，同样，需要根据id进行删除： 语法 1DELETE /索引库名/类型名/id值 示例： 1DELETE /qf/items/3 结果： 1234567891011121314&#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 3, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 2, &quot;_primary_term&quot;: 3&#125; 3.查询我们从4块来讲查询： 基本查询 _source过滤 结果过滤 高级查询 排序 3.1.基本查询： 基本语法 12345678GET /索引库名/_search&#123; "query":&#123; "查询类型":&#123; "查询条件":"查询条件值" &#125; &#125;&#125; 这里的query代表一个查询对象，里面可以有不同的查询属性 查询类型： 例如：match_all， match，term ， range 等等 查询条件：查询条件会根据类型的不同，写法也有差异，后面详细讲解 3.1.1 查询所有（match_all) 示例： 123456GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; took：查询花费时间，单位是毫秒 time_out：是否超时 _shards：分片信息 hits：搜索结果总览对象 total：搜索到的总条数 max_score：所有结果中文档得分的最高分 hits：搜索结果的文档对象数组，每个元素是一条搜索到的文档信息 _index：索引库 _type：文档类型 _id：文档id _score：文档得分 _source：文档的源数据 3.1.2 匹配查询（match）我们先加入一条数据，便于测试： 123456789POST /qf/items/3&#123; "title":"apple7plus", "sellpoint":"", "images":"http://www.images.com/003.jpg", "price":5899.00, "count": 20, "status":1&#125; or关系 match类型查询，会把查询条件进行分词，然后进行查询,多个词条之间是or的关系 12345678GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;oppo002&quot; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; and关系 某些情况下，我们需要更精确查找，我们希望这个关系变成and，可以这样做： 12345678GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&#123;&quot;query&quot;:&quot;oppo002&quot;,&quot;operator&quot;:&quot;and&quot;&#125; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.8630463, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8630463, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; or和and之间？ 在 or 与 and 间二选一有点过于非黑即白。 如果用户给定的条件分词后有 5 个查询词项，想查找只包含其中 4 个词的文档，该如何处理？将 operator 操作符参数设置成 and 只会将此文档排除。 有时候这正是我们期望的，但在全文搜索的大多数应用场景下，我们既想包含那些可能相关的文档，同时又排除那些不太相关的。换句话说，我们想要处于中间某种结果。 match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量： 例如： 123456789101112GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&#123; &quot;query&quot;:&quot;oppo002&quot;, &quot;minimum_should_match&quot;:&quot;50%&quot; &#125; &#125;&#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.8630463, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8630463, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; 搜索语句可以分为2个词，如果使用and关系，需要同时满足2个词才会被搜索到。这里我们采用最小匹配数：50%，那么也就是说只要匹配到总词条数量的50%即可，这里2*50% 约等于1。所以只要包含1个词条就算满足条件了。 3.1.3 多字段查询（multi_match）multi_match与match类似，不同的是它可以在多个字段中查询 123456789101112GET /qf/_search&#123; "query":&#123; "multi_match":&#123; "query":"oppo拍照", "fields":["title","sellpoint"] &#125;&#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; 本案例会在title中和sellpoint中查找是否带有：” oppo拍照”。 3.1.5 多词条精确匹配(terms)terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件： 1234567891011GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;price&quot;:[1299,5899] &#125;&#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;apple7plus&quot;, &quot;sellpoint&quot;: &quot;&quot;, &quot;images&quot;: &quot;http://www.images.com/003.jpg&quot;, &quot;price&quot;: 5899, &quot;count&quot;: 20, &quot;status&quot;: 1 &#125; &#125; ] &#125;&#125; 3.2.结果过滤默认情况下，elasticsearch在搜索的结果中，会把文档中保存在_source的所有字段都返回。 如果我们只想获取其中的部分字段，我们可以添加_source的过滤 3.2.1.直接指定字段示例： 123456789GET /qf/_search&#123; &quot;_source&quot;: [&quot;title&quot;,&quot;price&quot;], &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: 1299 &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;price&quot;: 1299, &quot;title&quot;: &quot;oppo002&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;price&quot;: 1299, &quot;title&quot;: &quot;oppo&quot; &#125; &#125; ] &#125;&#125; 只会返回”_source”属性中指定的字段 3.2.2.指定includes和excludes我们也可以通过： includes：来指定想要显示的字段 excludes：来指定不想要显示的字段 二者都是可选的。 示例： 1234567891011GET /qf/_search&#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [&quot;title&quot;,&quot;price&quot;] &#125;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: 1299 &#125; &#125;&#125; 结果与上一个案例一致： 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;price&quot;: 1299, &quot;title&quot;: &quot;oppo002&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;price&quot;: 1299, &quot;title&quot;: &quot;oppo&quot; &#125; &#125; ] &#125;&#125; excludes：来指定不想要显示的字段 例如： 1234567891011GET /qf/_search&#123; &quot;_source&quot;: &#123; &quot;excludes&quot;: [&quot;title&quot;,&quot;price&quot;] &#125;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: 1299 &#125; &#125;&#125; 结果：title和price字段没有显示出来 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot; &#125; &#125; ] &#125;&#125; 3.3 高级查询3.3.1 布尔组合（bool)bool把各种其它查询通过must（与）、must_not（非）、should（或）的方式进行组合 例如： 12345678910111213141516171819202122GET /qf/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match&quot;: &#123; &quot;title&quot;: &quot;oppo&quot; &#125;&#125; ], &quot;must_not&quot;: [ &#123;&quot;match&quot;: &#123; &quot;sellpoint&quot;: &quot;手机&quot; &#125;&#125; ], &quot;should&quot;: [ &#123;&quot;match&quot;: &#123; &quot;sellpoint&quot;: &quot;拍照&quot; &#125;&#125; ] &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.8630463, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8630463, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.8630463, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; 3.3.2 范围查询(range)range 查询找出那些落在指定区间内的数字或者时间 1234567891011GET /qf/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;:&#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 4000 &#125; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; range查询允许以下字符： 操作符 说明 gt 大于 gte 大于等于 lt 小于 lte 小于等于 g:great t:then e:equals 3.3.3 模糊查询(fuzzy)fuzzy 查询是 term 查询的模糊等价。它允许用户搜索词条与实际词条的拼写出现偏差，但是偏差的编辑距离不得超过2： 12345678GET /qf/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;:&#123; &quot;title&quot;: &quot;oapo&quot; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.21576157, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.21576157, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.21576157, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; 我们发现通过：”oapo”也能查询到”oppo”。 我们可以通过fuzziness来指定允许的编辑距离： 1234567891011GET /qf/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;:&#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;oapo&quot;, &quot;fuzziness&quot;:1 &#125; &#125; &#125;&#125; 结果也能查询出：oppo手机 the number of one character changes that need to be made to one string to make it the same as another string.值：0, 1, 2，允许的最大edits.值：AUTO，依据term长度。0..2 必须完全匹配;3..5 最大允许1个; oppo &gt;5 最大允许2个 3.4 过滤(filter) 条件查询中进行过滤 所有的查询都会影响到文档的评分及排名。如果我们需要在查询结果中进行过滤，并且不希望过滤条件影响评分，那么就不要把过滤条件作为查询条件来用。而是使用filter方式： 先在插入一条数据 123456789POST /qf/items/8&#123; &quot;title&quot;:&quot;vivo手机&quot;, &quot;sellpoint&quot;:&quot;vivo手机拍照手机拍照更清晰&quot;, &quot;images&quot;:&quot;http://www.images.com/004.jpg&quot;, &quot;price&quot;:2199.00, &quot;count&quot;:30, &quot;status&quot;:1&#125; 测试： 1234567891011121314151617181920GET /qf/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 3000 &#125; &#125; &#125; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.8630463, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8630463, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125; &#125; ] &#125;&#125; 如果一次查询只有过滤，没有查询条件，不希望进行评分，我们可以使用constant_score取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 12345678910GET /qf/_search&#123; "query":&#123; "constant_score": &#123; "filter": &#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3000.00&#125;&#125; &#125; &#125;&#125;&#125; 结果： 123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;vivo手机&quot;, &quot;sellpoint&quot;: &quot;vivo手机拍照手机拍照更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/004.jpg&quot;, &quot;price&quot;: 2199, &quot;count&quot;: 30, &quot;status&quot;: 1 &#125; &#125; ] &#125;&#125; 3.5 排序3.4.1 单字段排序sort 可以让我们按照不同的字段进行排序，并且通过order指定排序的方式 12345678910111213141516GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;sellpoint&quot;: &quot;拍照&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;price&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;vivo手机&quot;, &quot;sellpoint&quot;: &quot;vivo手机拍照手机拍照更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/004.jpg&quot;, &quot;price&quot;: 2199, &quot;count&quot;: 30, &quot;status&quot;: 1 &#125;, &quot;sort&quot;: [ 2199 ] &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;sort&quot;: [ 1299 ] &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;sort&quot;: [ 1299 ] &#125; ] &#125;&#125; 3.4.2 多字段排序假定我们想要结合使用 price和 _score（得分） 进行查询，并且匹配的结果首先按照价格排序，然后按照相关性得分排序： 1234567891011121314151617181920GET /qf/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;: &#123; &quot;sellpoint&quot;: &quot;手机拍照&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;price&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;,&#123; &quot;_score&quot;:&#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&#123; &quot;took&quot;: 19, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.5822514, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;vivo手机&quot;, &quot;sellpoint&quot;: &quot;vivo手机拍照手机拍照更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/004.jpg&quot;, &quot;price&quot;: 2199, &quot;count&quot;: 30, &quot;status&quot;: 1 &#125;, &quot;sort&quot;: [ 2199, 1.5822514 ] &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;sort&quot;: [ 1299, 0.5753642 ] &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;sort&quot;: [ 1299, 0.5753642 ] &#125; ] &#125;&#125; 3.6高亮显示123456789101112GET /qf/_search&#123; "query":&#123; "match": &#123; "title": "oppo" &#125; &#125;, "highlight": &#123; "fields": &#123;"title": &#123;&#125;&#125; &#125;&#125; 结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;took&quot;: 46, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 3, &quot;successful&quot;: 3, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo002&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0002.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;&lt;em&gt;oppo&lt;/em&gt;002&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;qf&quot;, &quot;_type&quot;: &quot;items&quot;, &quot;_id&quot;: &quot;c9uy1GoBht3nCF-2YSOO&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;oppo&quot;, &quot;sellpoint&quot;: &quot;拍照更更清晰&quot;, &quot;images&quot;: &quot;http://www.images.com/0001.jpg&quot;, &quot;price&quot;: 1299 &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;&lt;em&gt;oppo&lt;/em&gt;&quot; ] &#125; &#125; ] &#125;&#125; 我们发现每条查询出来的结果都有highlight 属性。该属性中有需要高亮显示的字段，并且使用标签，展示到前端会倾斜显示，可以适当加一下css样式使关键字更明显的展示。 4. 聚合aggregations聚合函数 avg count sum min max 聚合可以让我们极其方便的实现对数据的统计、分析。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。 4.1 基本概念Elasticsearch中的聚合，包含多种类型，最常用的两种，一个叫桶，一个叫度量： 桶（bucket）,阿里文档称之为存储单元 桶的作用，是按照某种方式对数据进行分组，每一组数据在ES中称为一个桶， 例如我们根据国籍对人划分，可以得到中国桶、英国桶，日本桶……或者我们按照年龄段对人进行划分：010,1020,2030,3040等。 跟mysql数据库的分组是一个概念。 Elasticsearch中提供的划分桶的方式有很多： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一个存储单元 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一个存储单元 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分 …… group by dept_id; 综上所述，我们发现bucket aggregations 只负责对数据进行分组，并不进行计算，因此往往bucket中往往会嵌套另一种聚合：metrics aggregations即度量 度量（metrics） 分组完成以后，我们一般会对组中的数据进行聚合运算，例如求平均值、最大、最小、求和等，这些在ES中称为度量 比较常用的一些聚合度量方式： Avg Aggregation：求平均值 Max Aggregation：求最大值 Min Aggregation：求最小值 Percentiles Aggregation：求百分比 Stats Aggregation：同时返回avg、max、min、sum、count等 Sum Aggregation：求和 Top hits Aggregation：求前几 Value Count Aggregation：求总数 …… 为了测试聚合，我们先批量导入一些数据 创建索引： 12345678910111213141516171819202122232425PUT /users&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 &#125;, &quot;mappings&quot;: &#123; &quot;transactions&quot;: &#123; &quot;properties&quot;: &#123; &quot;username&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;sex&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;provinceName&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;joindate&quot;:&#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 在ES中，需要进行聚合、排序、过滤的字段其处理方式比较特殊，因此不能被分词。这里我们将sex和provinceName这两个文字类型的字段设置为keyword类型，这个类型不会被分词，将来就可以参与聚合 批量导入测试数据： 1234567891011121314151617POST /users/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "age" : 21, "username" : "jack", "sex" : "男","provinceName":"北京", "joindate" : "2019-05-20" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 18, "username" : "rose", "sex" : "女","provinceName":"河北", "joindate" : "2018-11-15" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 30, "username" : "linda", "sex" : "女", "provinceName":"河南","joindate" : "2018-07-14" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 19, "username" : "bob", "sex" : "男","provinceName":"湖北","joindate" : "2018-09-09" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 12, "username" : "grace", "sex" : "女","provinceName":"北京", "joindate" : "2018-07-27" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 32, "username" : "joe", "sex" : "男","provinceName":"北京", "joindate" : "2018-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 9, "username" : "lucy", "sex" : "女","provinceName":"河北", "joindate" : "2018-09-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "age" : 22, "username" : "alice", "sex" : "女","provinceName":"河南", "joindate" : "2019-05-30" &#125; Bulk 一次请求 多次操作 1、批量创建，一个index，多个document 任意一个操作失败，是不会影响其他的操作 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142&#123; &quot;took&quot;: 28, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;n-8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;oO8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 1, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;oe8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 2, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;ou8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 3, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;o-8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 4, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;pO8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 5, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;pe8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 6, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;users&quot;, &quot;_type&quot;: &quot;transactions&quot;, &quot;_id&quot;: &quot;pu8k2WoBXNgExbudG7gQ&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 7, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125; ]&#125; 说明全部创建成功。 4.2 聚合为桶首先，我们按照 用户的性别来分。 1234567891011GET /users/_search&#123; "size": 0, "aggs": &#123; "sex": &#123; "terms": &#123; "field": "sex" &#125; &#125; &#125;&#125; size： 查询条数，这里设置为0，因为我们不关心搜索到的数据，只关心聚合结果，提高效率 aggs：声明这是一个聚合查询，是aggregations的缩写 popular_colors：给这次聚合起一个名字，任意。 terms：划分桶的方式，这里是根据词条划分 field：划分桶的字段 结果： 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;sex&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;女&quot;, &quot;doc_count&quot;: 5 &#125;, &#123; &quot;key&quot;: &quot;男&quot;, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; hits：查询结果为空，因为我们设置了size为0 aggregations：聚合的结果 popular_colors：我们定义的聚合名称 buckets：查找到的桶，每个不同的color字段值都会形成一个桶 key：这个桶对应的color字段的值 doc_count：这个桶中的文档数量 通过聚合的结果我们发现，目前红色的小车比较畅销！ 4.3 桶内度量前面的例子告诉我们每个桶里面的文档数量，这很有用。 但通常，我们的应用需要提供更复杂的文档度量。 例如，人员的平均年龄？ 因此，我们需要告诉Elasticsearch使用哪个字段，使用何种度量方式进行运算，这些信息要嵌套在桶内，度量的运算会基于桶内的文档进行 现在，我们为刚刚的聚合结果添加 求平均年龄的度量： 123456789101112131415161718GET /users/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;sex&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;sex.keyword&quot; &#125;, &quot;aggs&quot;:&#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;sex&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;女&quot;, &quot;doc_count&quot;: 5, &quot;avg_age&quot;: &#123; &quot;value&quot;: 18.2 &#125; &#125;, &#123; &quot;key&quot;: &quot;男&quot;, &quot;doc_count&quot;: 3, &quot;avg_age&quot;: &#123; &quot;value&quot;: 24 &#125; &#125; ] &#125; &#125;&#125; aggs：我们在上一个aggs(popular_colors)中添加新的aggs。可见度量也是一个聚合 avg_age：聚合的名称 avg：度量的类型，这里是求平均值 field：度量运算的字段 4.4 桶内嵌套桶刚刚的案例中，我们在桶内嵌套度量运算。事实上桶不仅可以嵌套运算， 还可以再嵌套其它桶。也就是说在每个分组中，再分更多组。 比如：我们想统计每种性别的用户中，分别属于哪个省份，按照人员的省份再次划分。 12345678910111213141516171819202122232425GET /users/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sex&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;sex&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125;, &quot;marker&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;provinceName&quot; &#125; &#125; &#125; &#125; &#125;&#125; 原来的sex桶和avg计算我们不变 maker：在嵌套的aggs下新添一个桶，叫做maker terms：桶的划分类型依然是词条 filed：这里根据make字段进行划分 结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;sex&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;女&quot;, &quot;doc_count&quot;: 5, &quot;avg_age&quot;: &#123; &quot;value&quot;: 18.2 &#125;, &quot;maker&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;河北&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;河南&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;北京&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;男&quot;, &quot;doc_count&quot;: 3, &quot;avg_age&quot;: &#123; &quot;value&quot;: 24 &#125;, &quot;maker&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;北京&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;湖北&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; ] &#125; &#125;&#125; 4.5.划分桶的其它方式前面讲了，划分桶的方式有很多，例如： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 刚刚的案例中，我们采用的是Terms Aggregation，即根据词条划分桶。 接下来，我们再学习几个比较实用的： histogram是把数值类型的字段，按照一定的阶梯大小进行分组。你需要指定一个阶梯值（interval）来划分阶梯大小。 举例： 比如你有价格字段，如果你设定interval的值为200，那么阶梯就会是这样的： 0，200，400，600，… 上面列出的是每个阶梯的key，也是区间的启点。 如果一件商品的价格是450，会落入哪个阶梯区间呢？计算公式如下： 1bucket_key = Math.floor((value - offset) / interval) * interval + offset value：就是当前数据的值，本例中是450 offset：起始偏移量，默认为0 interval：阶梯间隔，比如200 因此你得到的key = Math.floor((450 - 0) / 200) * 200 + 0 = 400 操作一下： 比如，我们对用户的年龄分组，指定间隔interval为5： 123456789101112GET /users/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;interval&quot;: 5 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;price&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 5, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: 10, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: 15, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: 20, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: 25, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key&quot;: 30, &quot;doc_count&quot;: 2 &#125; ] &#125; &#125;&#125; 你会发现，中间有大量的文档数量为0 的桶，看起来很丑。 { “key”: 0, “doc_count”: 1 }, 我们可以增加一个参数min_doc_count为1，来约束最少文档数量为1，这样文档数量为0的桶会被过滤 示例： 12345678910111213GET /users/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;price&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;interval&quot;: 5, &quot;min_doc_count&quot;: 1 &#125; &#125; &#125;&#125; 如果你用kibana将结果变为柱形图，会更好看： 图形说明 1、Area ：用区块图来可视化多个不同序列的总体贡献。 2、Data ：用数据表来显示聚合的原始数据。其他可视化可以通过点击底部的方式显示数据表。 3、Line ：用折线图来比较不同序列。 4、Markdown ： 用 Markdown 显示自定义格式的信息或和你仪表盘有关的用法说明。 5、Metric ：用指标可视化在你仪表盘上显示单个数字。 6、Pie ：用饼图来显示每个来源对总体的贡献。 7、Tile map ：用瓦片地图将聚合结果和经纬度联系起来。 8、Timeseries ：计算和展示多个时间序列数据。 9、Vertical bar ：用垂直条形图作为一个通用图形。 例如： 4.5.2.范围分桶range范围分桶与阶梯分桶类似，也是把数字按照阶段进行分组，只不过range方式需要你自己指定每一组的起始和结束大小。]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop生态学习]]></title>
    <url>%2F2019%2F04%2F07%2FHadoop%E7%94%9F%E6%80%81%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1. 资源管理yarn 官方文档：https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html 2. 数据库HBase Hive和Hbase的区别：https://www.cnblogs.com/justinzhang/p/4273470.html HBase官方文档：http://hbase.apache.org/book.html#_overriding_configuration_starting_the_hbase_shell HBase学习：http://www.importnew.com/21958.html IBM HBASE资料：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-bigdata-hbase/index.html Impala与hive：https://blog.csdn.net/xiangxizhishi/article/details/77203478 Hive是一个基于Hadoop的查询工具，支持HQL语言（类SQL）查询。批处理方式，故不适用于实时业务。Impala也是一个基于Hadoop的查询工具，优点是速度快，支持SQL语义。两者可以配合使用。 3. hive 文件存储格式：https://blog.csdn.net/qq_31807385/article/details/84796880 表类型：https://blog.csdn.net/u010886217/article/details/83796151 Hive小文件优化：https://blog.csdn.net/djd1234567/article/details/51581201 4. kylinhttp://kylin.apache.org/cn/ https://www.jianshu.com/p/abd5e90ab051 5. Hadoop命令大全 FS Shell命令：https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html 用户命令：https://hadoop.apache.org/docs/r1.0.4/cn/commands_manual.html#jar 5.1 常用命令 查看目录下文件：/www/hadoop-2.7.4/bin/hadoop fs -ls -h /tmp/result/k2/mp 本地文件加载到HDFS：/www/hadoop-2.7.4/bin/hadoop fs -put $tfname /tmp/result/$tfname 6. HQL语法LOAD语句：https://blog.csdn.net/skywalker_only/article/details/36189415 7. MapReducer基础知识：https://www.jianshu.com/p/6162b787a428]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB原理]]></title>
    <url>%2F2019%2F04%2F05%2FMongoDB%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. 存储引擎 WiredTiger:https://yq.aliyun.com/articles/255163，支持文档级别的锁 MMAPv1:https://yq.aliyun.com/articles/400160?spm=a2c4e.11153940.blogcont2352.17.723d5d6cl55Dnp，最原始MongoDB的存储引擎 2. 存储结构https://www.cnblogs.com/shijingxiang/articles/5225980.html 3. 集群方案https://blog.csdn.net/madongyu1259892936/article/details/85626840 4. 使用疑难点 游标超时：https://blog.csdn.net/weixin_41287692/article/details/82804123 删除指定字段：https://codeday.me/bug/20170816/58017.html 12例子：db.mp_feature_00.update(&#123;&#125;,&#123;$unset: &#123;frame_id: 1&#125;&#125;, false, true) 修改字段类型：https://codeday.me/bug/20170602/24255.html 12例子：db.mp_feature_00.find(&#123;&quot;feed_id&quot;: &#123;$type: 18&#125;&#125;).forEach(function(x)&#123;x.feed_id=x.feed_id+&quot;&quot;;x.tag_time=new NumberInt(1559120665);db.mp_feature_00.save(x)&#125;) 数组修改器：https://blog.csdn.net/frightingforambition/article/details/50684394 重建索引：https://docs.mongodb.com/manual/reference/method/db.collection.reIndex/ 5. 索引类型https://itbilu.com/database/mongo/VJG8tei0g.html 6. 游标超时问题现象：当从数据库中查询的数据比较多时，查询时间较长，会出现游标超时报错。 1CursorNotFound(u&apos;cursor id 654096680340 not found&apos;,) 解决方法1：https://segmentfault.com/a/1190000018475477，尝试过还是有问题。 7. 数据备份https://www.jianshu.com/p/5c8307e2b1ce MongoExport：https://www.cnblogs.com/limingluzhu/p/4323146.html 8. 锁机制https://blog.51cto.com/raugher/1678704https://www.cnblogs.com/duanxz/p/10737548.html 目前MongoDB 3.4版本后支持文档级别的锁，单条数据被一个进程写入时候，其他进程不能写入该数据。 9. 事务机制http://database.51cto.com/art/201811/586979.htm MongoDB 4.0版本后支持事务事务是原子操作，可以支持回滚，把一连串操作打包。 10. Change Loghttps://docs.mongodb.com/manual/changeStreams/ 11. mapreduce计算模型实现参考链接：https://www.runoob.com/mongodb/mongodb-map-reduce.html 12345678db.feed_feature_00.mapReduce( function() &#123;emit(this.mlevel_time,this.create_time);&#125;, function(key,values) &#123;return key-values&#125;, &#123; query: &#123;&quot;create_time&quot;: &#123;$gte: 1562774400, $lte: 1562778000&#125;, &quot;mlevel&quot;: &#123;$exists: true&#125;, &quot;mlevel_time&quot;: &#123;$exists: true&#125;&#125;, out: &#123; inline: 1 &#125; &#125;).find() 12. Java写入mongo的几种方式https://www.cnblogs.com/tbyang/p/4203046.html 13. mongo监控 13.1 mongotop工具 1mongotop --host=&quot;10.10.32.225:3006&quot; --username=&quot;cloud_vision&quot; --password=&quot;Uh2i9Nal2sf&quot; --authenticationDatabase=&quot;admin&quot; 13.2 mongostat工具 1mongostat --host=&quot;10.10.32.225:3006&quot; --username=&quot;cloud_vision&quot; --password=&quot;Uh2i9Nal2sf&quot; --authenticationDatabase=&quot;admin&quot; 其他监控工具参考：http://www.mongoing.com/docs/administration/monitoring.html]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB._id踩坑]]></title>
    <url>%2F2019%2F04%2F02%2FMongoDB-id%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[1. 上下文 业务中使用MongoDB作为存储数据库，随着数据量上涨，假设业务用feed_id唯一标识内容，库里面会存了多条feed内容。 原因分析：即使服务中写库时候有对唯一性的验证，先查库，如果没有feed_id insert整条数据，有feed_id则update某些值。在并发很大的情况下，A/B两个节点同时查库发现都没有数据，两边都insert，就会导致一个feed_id存在多条记录。 2. 为啥会采坑 数据库：mongo自身有主键”_id”，来确保集合里面每个文档都能被唯一标识。这个”_id”如果在程序中不指定，MongoDB将自动生成不重复的24位字符串。至于为啥”_id”不是像MySQL等自增的，主要是自增主键通不起来费时费力，不适用为分布式而设计的MongoDB。 个人：开发时间有限，对MongoDB了解有限。—-真实原因是能力有限 3. 解决方案3.1 理想情况—-业务还未接入1方案一：直接把业务id 比如feed_id存成MongoDB主键&quot;_id&quot;，分布式集群分片和普通集群都适用。推荐指数5星 1方案二：把业务id 比如feed_id建唯一索引。推荐指数5星 3.2 实际情况1方案一：更改服务以及历史数据，&quot;_id&quot;和&quot;feed_id&quot;都存储唯一标识ID，不需要对数据库做更改，&quot;_id&quot;存业务ID保证唯一性，&quot;feed_id&quot;保证现有业务使用方式不受影响。推荐指数5星 1方案二：定时任务检查feed_id重复情况，重复时候合并数据并删除另外一条 ,对业务影响最小。推荐指数4星 1方案三：重新搭建MongoDB数据库，把数据迁入以及业务迁入，缺点是费时费力，线上有受影响的风险。推荐指数4星 1方案四：使用MongoDB重建索引命令，重建feed_id 唯一索引，参考https://docs.mongodb.com/manual/reference/method/db.collection.reIndex/，缺点是重建过程不能background执行，重建原理是先删除现有索引再重建，线上影响很大。推荐指数3星]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的难点]]></title>
    <url>%2F2019%2F03%2F22%2FGolang%E7%9A%84%E9%9A%BE%E7%82%B9%2F</url>
    <content type="text"><![CDATA[踩过的坑：http://wudaijun.com/2018/08/go-is-not-good/ 入门到精通：https://www.kancloud.cn/cserli/golang/530431 1. for range的坑https://www.jianshu.com/p/04f117c22873 2. select关键词http://www.runoob.com/go/go-select-statement.html 3. new和make的区别https://studygolang.com/articles/3496 4. type的使用https://blog.csdn.net/hzwy23/article/details/79890778 5. cap的使用https://my.oschina.net/joeyjava/blog/504249 6. switch和select的区别https://studygolang.com/articles/10285 7. Go的内存分配https://juejin.im/post/5c888a79e51d456ed11955a8?utm_source=gold_browser_extension 8. Go的垃圾回收https://blog.csdn.net/u010649766/article/details/80582153https://studygolang.com/articles/7366 9. build和install的区别https://blog.csdn.net/smile_yangyue/article/details/80680050 10. 异常处理 defer捕获异常：https://www.cnblogs.com/ghj1976/archive/2013/02/11/2910114.html 错误处理：https://www.flysnow.org/2019/01/01/golang-error-handle-suggestion.html 11. 深拷贝 copier库：https://github.com/jinzhu/copier 自己实现深拷贝 12345678910111213141516171819func DeepCopy(value interface&#123;&#125;) interface&#123;&#125; &#123; if valueMap, ok := value.(map[string]interface&#123;&#125;); ok &#123; newMap := make(map[string]interface&#123;&#125;) for k, v := range valueMap &#123; newMap[k] = DeepCopy(v) &#125; return newMap &#125; else if valueSlice, ok := value.([]interface&#123;&#125;); ok &#123; newSlice := make([]interface&#123;&#125;, len(valueSlice)) for k, v := range valueSlice &#123; newSlice[k] = DeepCopy(v) &#125; return newSlice &#125; return value&#125; 12. 优化http结果解析https://blog.thinkeridea.com/201901/go/you_ya_de_du_qu_http_qing_qiu_huo_xiang_ying_de_shu_ju.html 13. 100 continue问题https://blog.csdn.net/zhangzq86/article/details/53023689]]></content>
      <categories>
        <category>开发语言</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang基础知识]]></title>
    <url>%2F2019%2F03%2F22%2FGolang%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1. 关键词 1.1 defer关键词：https://blog.csdn.net/kiloveyousmile/article/details/80298697 1.2 iota和const关键词：http://www.runoob.com/go/go-constants.html 判断struct和map相等reflect.deepequal：https://studygolang.com/articles/2194 2. GoLang HTTP请求的bug https://gocn.vip/article/314 3. GoLang 操作数据库 3.1 MongoDB操作： 4. chan使用https://sanyuesha.com/2017/08/03/go-channel/ 5. print复杂对象 https://blog.cyeam.com/golang/2017/03/06/go-fmt-v 6. 单元测试工具GoConvey：https://blog.csdn.net/zwqjoy/article/details/79474196 7. 动态数组的创建http://www.h1z166.com/articles/2017/10/09/1507525187733.html 8. buffer缓冲器https://juejin.im/post/5bf909cb51882521c8114523]]></content>
      <categories>
        <category>开发语言</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker原理]]></title>
    <url>%2F2018%2F10%2F24%2FDocker%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. Docker学习文档https://yeasy.gitbooks.io/docker_practice/content/kubernetes/concepts.html 2. Dockerfile书写规范https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ 3. Docker对硬件的调度原理https://www.linuxidc.com/Linux/2017-10/147488.htm 4. Kubernetes的共享GPU集群调度https://blog.csdn.net/yunqiinsight/article/details/87694685]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之视觉]]></title>
    <url>%2F2018%2F10%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%A7%86%E8%A7%89%2F</url>
    <content type="text"><![CDATA[技术博客：https://zhuanlan.zhihu.com/vision-graffiti 网络模型介绍：http://blog.csdn.net/shuzfan/article/details/50037273 AI学习资料：https://study.163.com/courses-search?keyword=%E5%90%B4%E6%81%A9%E8%BE%BE 1. 相似检索 faiss：https://www.leiphone.com/news/201703/84gDbSOgJcxiC3DW.html ahash\dhash\phash：http://blog.sina.com.cn/s/blog_56fd58ab0102xpqf.html 2. 乘积量化PQ https://blog.csdn.net/guanyonglai/article/details/78468673 3. 深度学习框架mxnet mxnet安装过程：http://mxnet.incubator.apache.org/install/index.html mxnet使用过程：http://blog.csdn.net/mydear_11000/article/details/51144183 mxnet API :http://www.infoq.com/cn/articles/an-introduction-to-the-mxnet-api-part04?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk https://blog.csdn.net/qq_25491201/article/details/51386435]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>深度学习，视觉算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka队列]]></title>
    <url>%2F2018%2F07%2F19%2FKafka%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[1. 基础知识2. 疑难点2.1 顺序保证Kafka分布式的单位是partition，同一个partition用一个write ahead log组织，所以可以保证FIFO的顺序。 实际场景一般都是分布式的（既存在多个partition），多个Partition时，不能保证Topic级别的数据有序性。需要使用其他方式来实现topic级别的顺序： A. 可以通过message key取模指定写入的partition，能保证同一个message key只会经过同一个partition，从而保证了该message key处理的顺序性。应用实例：爱奇艺搜索架构 B. 另外版本控制（时间版本）是一种在业务上控制时序的方法，带上时间戳入库的时候检查时间版本是否为历史版本，历史版本则舍弃。 参考链接：https://blog.csdn.net/thekenofdis/article/details/79973589]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL原理]]></title>
    <url>%2F2018%2F06%2F09%2FMySQL%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. MySQL BinLog binlog命令行查看：https://www.cnblogs.com/martinzhang/p/3454358.html binlog同步工具：https://github.com/alibaba/canal 2. MySQL 语句执行过程 执行过程：https://blog.csdn.net/finalkof1983/article/details/84450896 3. 索引 BTree索引和hash索引：https://blog.csdn.net/oChangWen/article/details/54024063 4. 锁机制 概念：https://www.jianshu.com/p/f5ff017db62a]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定时任务]]></title>
    <url>%2F2018%2F05%2F18%2F%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[1. crontab 使用形式：基于Linux命令开启和管理，并支持多用户，方便快捷。 常用命令 编辑任务内容：crontab -e 显示任务列表：crontab -l 查看crontab服务状态：/sbin/service crond status 重启crontab服务：/sbin/service crond restart 其他参考：https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html 2. apscheduler使用形式：Python开发，需要部署单独的服务。 例子： 12345678910111213141516# -*- coding:utf-8 -*-from apscheduler.schedulers.blocking import BlockingSchedulerfrom inspector.mlevel_inspector import mlevel_lack_handler&quot;&quot;&quot;@author zhaowujinCreated 2019/05/13&quot;&quot;&quot;scheduler = BlockingScheduler()scheduler.add_job(mlevel_lack_handler, &apos;cron&apos;, hour=8, minute=30)print &apos;K2 Cron start sucess!!&apos;scheduler.start() 其他参考：https://zhuanlan.zhihu.com/p/46948464]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令之文件操作]]></title>
    <url>%2F2018%2F04%2F25%2FLinux%E5%91%BD%E4%BB%A4%E4%B9%8B%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. grep命令 1.1 显示指定字符所在的行： 1234567grep -n \&quot;req_time\&quot;:3 --color=auto info.loggrep -n &apos;video_url_list\=1027127974&apos; --color=auto info.logtail -f nohup.out | grep -n &quot;\[503\]&quot; --color=autotail -f nohup.out | grep -n &quot;.*;.*&quot; --color=auto 1.2 统计符合条件的行数： 1grep -o &apos;&quot;req_time&quot;:5&apos; info.log | wc -l 2. awk命令 2.1 Java日志请求时间平均值：1cat info.log | awk -F &apos;\&quot;req_time\&quot;:&apos; &apos;&#123;print $2&#125;&apos; | awk -F &apos;&#125;&apos; &apos;&#123;print $1&#125;&apos; | awk &apos;&#123;sum+=$1&#125; END &#123;print &quot;API响应时间 = &quot;, sum/NR, &quot;秒&quot;&#125;&apos; 3. cat命令 3.1 多文件去除重复行： 使用介绍：https://blog.csdn.net/hanglinux/article/details/50569011 1cat FeedId_A_data.dat FeedId_New_data.dat | sort | uniq -u &gt; FeedId_other_data.dat 4. sed命令 4.1 正则替换文件内容： 使用介绍：https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html 1sed -i &apos;s/\&quot;\&#125;//g&apos; Back_SABC_feed_90.json 5. cp命令 5.1 两个目录的合并： 使用介绍：https://www.linuxidc.com/Linux/2012-12/75974.htm 1cp -frap new/* test/ 清空大文件：https://linux.cn/article-8024-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用技巧]]></title>
    <url>%2F2018%2F04%2F20%2FLinux%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[根据文件名查看文件位置：find / -name httpd.conf 查看系统版本：lsb_release -a 查看当前目录下文件大小：ll -h 显示目前所有文件系统的可用空间及使用情形：df -h 查询文件或文件夹的磁盘使用空间：du -h –max-depth=1 /du -h /usr/local/lib/python2.7/dist-packages/mxnet (当前文件夹下所有文件夹的大小) 查找命令所在位置：which echo 根据PID查找程序的全路径：http://blog.csdn.net/lsbhjshyn/article/details/18764613 net.core.somaxconn参数：https://www.cnblogs.com/jeffen/p/6066696.html etcd学习文档：http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle ifconfig 直接输出ip：ifconfig eth0 |grep ‘addr:[0-9]………….[0-9]’ -o|awk -F: ‘{print $2}’ifconfig eth0 |grep -E ‘addr:[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}’ -o |awk -F: ‘{print $2}’ifconfig eth0 |awk -F: ‘/[[:digit:]]/‘’{print $2}’|awk ‘{print $1}’|sed -n ‘2p’ifconfig eth0 |grep ‘[0-9]………….[0-9]’ -o|sed -n ‘2p’ifconfig eth0 |grep ‘[0-9].[0-9].[0-9].[0-9].[0-9].[0-9].[0-9].[0-9]’ |awk -F: ‘{print $2}’|awk ‘{print $1}’ifconfig eth0 |sed -n ‘2p’|awk -F: ‘{print $2}’|awk ‘{print $1}’ifconfig eth0 |sed -n ‘2p’ |grep ‘addr:[[0-9]………….[0-9]’ -o |awk -F: ‘{print $2}’ifconfig eth0 |awk ‘/addr:[[:digit:]]/‘’{print $2}’ |sed ‘s/addr://g’ifconfig eth0 |grep -E ‘addr:[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}’ -o |sed ‘s/addr://g’ifconfig eth0 |grep ‘addr:[0-9]…………[0-9]’ -o|sed -e ‘2p’ -e ‘s/addr://g’ifconfig eth0 |sed -rn ‘s#^.dr:(.)B.*$#\1#gp’ifconfig eth0 |awk -F ‘[ :]+’ ‘NR==2 {print $4}’ 查看CPU个数：top 按1 查看内存情况：head /proc/meminfo free -h httpstat工具使用：https://linux.cn/article-8039-1.html 修改文件所有者：sudo chown zwj1 ./ -R 修改文件所在组：sudo chgrp zwj1 ./ -R mysql python客户端安装：https://www.cnblogs.com/elseliving/p/8060393.html 配置共享盘：http://blog.topspeedsnail.com/archives/908 避免多次输出source命令的方法:https://bbs.csdn.net/topics/392114422 md5值获取：md5sum 需要测试文件 &gt; 保存结果文件 查看网络延迟：ping -c 10 192.168.7.122 找到文件位置：locate mysql_config CPU使用率查看：sudo pip install s-tuis-tui 安装lrzsz：sudo apt-get install lrzsz 僵尸进程：defunct 僵尸进程杀死： Linux网络抓包：安装https://blog.csdn.net/Canhui_WANG/article/details/78652033 12345使用：sudo tcpdump -i any port 8080 -w 8080.pcapsudo tcpdump -i bond0 host 10.10.64.20 and port 8080 -w 8080.pcap抓包文件保存在8080.pcap，通过查看抓包文件可以分析网络状况查询指定端口访问情况：netstat -ntap | grep 8080 | grep ESTA 统计文本行数：wc -l nohup.out 统计符合条件的行数：grep -o ‘“req_time”:5’ info.log | wc -l grep -o ‘“[[503]]”‘ nohup.out | wc -l nohup执行脚本输出慢：nohup python -u offline_db_pool_update.py &gt;&gt; nohup.out nohup使用：https://blog.csdn.net/cw370008359/article/details/51579721 time命令使用详解：https://man.linuxde.net/timehttps://man.linuxde.net/time watch命令详解：https://www.jb51.net/LINUXjishu/377400.html nc命令详解：https://www.runoob.com/linux/linux-comm-nc.html 12在192.168.2.34上： nc -l 1234在192.168.2.33上： nc 192.168.2.34 1234 telnet命令详解：https://www.linuxprobe.com/telnet-test-port.html 1查看端口是否可以访问：telnet 10.10.32.225 3006]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL自关联]]></title>
    <url>%2F2015%2F03%2F15%2FMySQL%E8%87%AA%E5%85%B3%E8%81%94%2F</url>
    <content type="text"><![CDATA[中国地名表实现自关联查询：定义表areas，结构如下:idatitlepid因为省没有所属的省份，所以可以填写为null城市所属的省份pid，填写省所对应的编号id这就是自关联，表中的某一列，关联了这个表中的另外一列，但是它们的业务逻辑含义是不一样的，城市信息的pid引用的是省信息的id在这个表中，结构不变，可以添加区县、乡镇街道、村社区等信息创建areas表的语句如下： 12345create table areas(aid int primary key,atitle varchar(20),pid int); 从sql文件中导入数据1source areas.sql; 查询一共有多少个省1select count(*) from areas where pid is null; 例1：查询省的名称为“山西省”的所有城市 123select city.* from areas as cityinner join areas as province on city.pid=province.aidwhere province.atitle='山西省'; 例2：查询市的名称为“广州市”的所有区县 123select dis.* from areas as disinner join areas as city on city.aid=dis.pidwhere city.atitle='广州市'; 在同一张表，通过自己和自己关联实现查询，省去了创建多张表。本例的中国地名表很有代表性，所以在此总结一下自关联的应用。]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL连接查询]]></title>
    <url>%2F2015%2F03%2F02%2FMySQL%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[当查询结果的列来源于多张表时，需要将多张表连接成一个大的数据集，再选择合适的列返回mysql支持三种类型的连接查询，分别为： 内连接查询：查询的结果为两个表匹配到的数据 左连接查询：查询的结果为两个表匹配到的数据，左表特有的数据，对于右表中不存在的数据使用null填充 右连接查询：查询的结果为两个表匹配到的数据，右表特有的数据，对于左表中不存在的数据使用null填充 语法 12select * from 表1inner或left或right join 表2 on 表1.列=表2.列 内连接：INNER JOIN ON 12345#连接查询：表示查询所有数据的非NULL部分#默认是内连接SELECT *FROM t_book, t_bookTypeWHERE t_book.bookTypeId = t_bookType.id; 1234567891011# 隐式连接# 给每张表起别名：SELECT tb.bookName, tb.price, tby.bookTypeNameFROM t_book as tb, t_bookType as tbyWHERE tb.bookTypeId = tby.id;# 显式连接SELECT tb.bookName, tb.price, tby.bookTypeNameFROM t_book as tbINNER JOIN t_bookType as tbyON tb.bookTypeId = tby.id; 交集： 属于A集合，也 属于B集合 的数据， A &amp; B并集： 属于A集合 和 属于B 的数据的全部集合， A | B（重复数据只保留一个） 左连接 ： LEFT JOIN ON 两张表连接查询，无论二者是否有数据缺失，一定会把左边的表数据全部显示出来 123456789SELECT *FROM t_bookLEFT JOIN t_bookTypeON t_book.bookTypeId = t_bookType.idSELECT tb.bookName, tb.price, tb.author, tby.bookTypeNameFROM t_book as tbLEFT JOIN t_bookType as tbyON tb.bookTypeId = tby.id 右连接： RIGHT JOIN ON 两张表连接查询，无论二者是否有数据缺失，一定会把右边的表数据全部显示出来 123456789SELECT *FROM t_bookRIGHT JOIN t_bookTypeON t_book.bookTypeId = t_bookType.idSELECT tb.bookName, tb.price, tb.author, tby.bookTypeNameFROM t_book as tbRIGHT JOIN t_bookType as tbyON tb.bookTypeId = tby.id 全连接： UNION 两张表连接查询，无论二者是否有数据缺失，一定会把右边的表数据全部显示出来 123456789SELECT tb.bookName, tb.price, tb.author, tby.bookTypeNameFROM t_book as tbLEFT JOIN t_bookType as tbyON tb.bookTypeId = tby.idUNIONSELECT tb.bookName, tb.price, tb.author, tby.bookTypeNameFROM t_book as tbRIGHT JOIN t_bookType as tbyON tb.bookTypeId = tby.id]]></content>
      <categories>
        <category>后端资源</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
</search>
